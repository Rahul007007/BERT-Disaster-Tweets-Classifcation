{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"References: \n- https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n- https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n- https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom wordcloud import STOPWORDS\nfrom collections import defaultdict\nimport random\nimport re\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-10T04:35:43.442075Z","iopub.execute_input":"2023-07-10T04:35:43.442523Z","iopub.status.idle":"2023-07-10T04:35:46.231932Z","shell.execute_reply.started":"2023-07-10T04:35:43.442485Z","shell.execute_reply":"2023-07-10T04:35:46.230707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.234538Z","iopub.execute_input":"2023-07-10T04:35:46.235007Z","iopub.status.idle":"2023-07-10T04:35:46.361105Z","shell.execute_reply.started":"2023-07-10T04:35:46.234967Z","shell.execute_reply":"2023-07-10T04:35:46.359742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0: Not a disaster\n1: Disaster","metadata":{}},{"cell_type":"code","source":"class_names =['Not a disaster', 'Disaster']","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.362765Z","iopub.execute_input":"2023-07-10T04:35:46.363134Z","iopub.status.idle":"2023-07-10T04:35:46.369273Z","shell.execute_reply.started":"2023-07-10T04:35:46.363104Z","shell.execute_reply":"2023-07-10T04:35:46.367937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.371985Z","iopub.execute_input":"2023-07-10T04:35:46.372410Z","iopub.status.idle":"2023-07-10T04:35:46.393335Z","shell.execute_reply.started":"2023-07-10T04:35:46.372373Z","shell.execute_reply":"2023-07-10T04:35:46.391854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.395185Z","iopub.execute_input":"2023-07-10T04:35:46.396220Z","iopub.status.idle":"2023-07-10T04:35:46.410147Z","shell.execute_reply.started":"2023-07-10T04:35:46.396176Z","shell.execute_reply":"2023-07-10T04:35:46.408817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balance_counts = df.groupby('target')['target'].agg('count').values\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['0'],\n    y=[balance_counts[0]],\n    text=[balance_counts[0]],\n    textposition='auto',\n))\nfig.add_trace(go.Bar(\n    x=['1'],\n    y=[balance_counts[1]],\n    text=[balance_counts[1]],\n    textposition='auto',\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px;\">Dataset distribution by target</span>'\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.412043Z","iopub.execute_input":"2023-07-10T04:35:46.412496Z","iopub.status.idle":"2023-07-10T04:35:46.576028Z","shell.execute_reply.started":"2023-07-10T04:35:46.412420Z","shell.execute_reply":"2023-07-10T04:35:46.574601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes are slightly imbalanced","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.577976Z","iopub.execute_input":"2023-07-10T04:35:46.578534Z","iopub.status.idle":"2023-07-10T04:35:46.610194Z","shell.execute_reply.started":"2023-07-10T04:35:46.578475Z","shell.execute_reply":"2023-07-10T04:35:46.609210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_count\ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ndf['url_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndf['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf['char_count'] = df['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:46.611704Z","iopub.execute_input":"2023-07-10T04:35:46.612303Z","iopub.status.idle":"2023-07-10T04:35:47.132415Z","shell.execute_reply.started":"2023-07-10T04:35:46.612269Z","shell.execute_reply":"2023-07-10T04:35:47.130954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Truncate some extreme values for better visuals ##\ndf['word_count'].loc[df['word_count']>60] = 60 #truncation for better visuals\ndf['char_count'].loc[df['char_count']>350] = 350 #truncation for better visuals\ndf['punctuation_count'].loc[df['punctuation_count']>10] = 10 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(20,30))\nsns.boxplot(x='target', y='word_count', data=df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='char_count', data=df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='punctuation_count', data=df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:47.134470Z","iopub.execute_input":"2023-07-10T04:35:47.135919Z","iopub.status.idle":"2023-07-10T04:35:48.271116Z","shell.execute_reply.started":"2023-07-10T04:35:47.135869Z","shell.execute_reply":"2023-07-10T04:35:48.269636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hashtag_count'].loc[df['hashtag_count']>60] = 60 #truncation for better visuals\ndf['mention_count'].loc[df['mention_count']>60] = 60 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(20,30))\n\nsns.boxplot(x='target', y='hashtag_count', data=df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of Hashtags in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='mention_count', data=df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of Mentions in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='url_count', data=df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\naxes[2].set_title(\"Number of URLs in each class\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:48.275810Z","iopub.execute_input":"2023-07-10T04:35:48.276429Z","iopub.status.idle":"2023-07-10T04:35:49.239828Z","shell.execute_reply.started":"2023-07-10T04:35:48.276392Z","shell.execute_reply":"2023-07-10T04:35:49.238797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exclude = string.punctuation\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', text)\n\ndef remove_punc(text):\n    return text.translate(str.maketrans('', '', exclude))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:49.241369Z","iopub.execute_input":"2023-07-10T04:35:49.241976Z","iopub.status.idle":"2023-07-10T04:35:49.248203Z","shell.execute_reply.started":"2023-07-10T04:35:49.241941Z","shell.execute_reply":"2023-07-10T04:35:49.246375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\ntext2 = '!hello *world@ 1'\ndf['text'] = df['text'].apply(remove_url)\n#df['text'] = df['text'].apply(remove_punc)\ntest_df['text'] = test_df['text'].apply(remove_url)\n#test_df['text'] = test_df['text'].apply(remove_punc)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:49.250272Z","iopub.execute_input":"2023-07-10T04:35:49.251266Z","iopub.status.idle":"2023-07-10T04:35:49.325403Z","shell.execute_reply.started":"2023-07-10T04:35:49.251218Z","shell.execute_reply":"2023-07-10T04:35:49.324286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"execution":{"iopub.status.busy":"2023-06-26T09:20:53.226485Z","iopub.execute_input":"2023-06-26T09:20:53.226950Z","iopub.status.idle":"2023-06-26T09:20:53.232733Z","shell.execute_reply.started":"2023-06-26T09:20:53.226897Z","shell.execute_reply":"2023-06-26T09:20:53.230889Z"}}},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:35:49.327001Z","iopub.execute_input":"2023-07-10T04:35:49.327745Z","iopub.status.idle":"2023-07-10T04:36:04.679752Z","shell.execute_reply.started":"2023-07-10T04:35:49.327711Z","shell.execute_reply":"2023-07-10T04:36:04.678506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:04.681957Z","iopub.execute_input":"2023-07-10T04:36:04.682352Z","iopub.status.idle":"2023-07-10T04:36:05.783427Z","shell.execute_reply.started":"2023-07-10T04:36:04.682321Z","shell.execute_reply":"2023-07-10T04:36:05.782114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_text = 'I will watch #Memento tonight!'\nbert_input = tokenizer(example_text, padding='max_length', max_length = 15,\n                      truncation = True, return_tensors = 'pt')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.785058Z","iopub.execute_input":"2023-07-10T04:36:05.785406Z","iopub.status.idle":"2023-07-10T04:36:05.809082Z","shell.execute_reply.started":"2023-07-10T04:36:05.785376Z","shell.execute_reply":"2023-07-10T04:36:05.807971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bert_input)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.810185Z","iopub.execute_input":"2023-07-10T04:36:05.810570Z","iopub.status.idle":"2023-07-10T04:36:05.847056Z","shell.execute_reply.started":"2023-07-10T04:36:05.810537Z","shell.execute_reply":"2023-07-10T04:36:05.845597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(bert_input['input_ids'][0]))\nprint(bert_input.input_ids) # id representation of each token\nprint(bert_input.token_type_ids) # a binary mask that identifies in which sequence a token belongs\nprint(bert_input.attention_mask) #  a binary mask that identifies whether a token is a real word or just padding. If the token contains [CLS], [SEP], or any real word, then the mask would be 1. Meanwhile, if the token is just padding or [PAD], then the mask would be 0.","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.848685Z","iopub.execute_input":"2023-07-10T04:36:05.849981Z","iopub.status.idle":"2023-07-10T04:36:05.859587Z","shell.execute_reply.started":"2023-07-10T04:36:05.849932Z","shell.execute_reply":"2023-07-10T04:36:05.857932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_text = tokenizer.decode(bert_input.input_ids[0])\nprint(example_text)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.861949Z","iopub.execute_input":"2023-07-10T04:36:05.862472Z","iopub.status.idle":"2023-07-10T04:36:05.873179Z","shell.execute_reply.started":"2023-07-10T04:36:05.862402Z","shell.execute_reply":"2023-07-10T04:36:05.871548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_input['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.875602Z","iopub.execute_input":"2023-07-10T04:36:05.876105Z","iopub.status.idle":"2023-07-10T04:36:05.888135Z","shell.execute_reply.started":"2023-07-10T04:36:05.876061Z","shell.execute_reply":"2023-07-10T04:36:05.886812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = torch.tensor(bert_input.input_ids)\nattention_mask = torch.tensor(bert_input.attention_mask)\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nlast_hidden_state, pooled_output = bert_model(input_ids=input_ids, attention_mask=attention_mask, return_dict =False)\n\nprint(last_hidden_state.shape)\nprint(bert_model.config.hidden_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:05.889761Z","iopub.execute_input":"2023-07-10T04:36:05.890142Z","iopub.status.idle":"2023-07-10T04:36:11.311843Z","shell.execute_reply.started":"2023-07-10T04:36:05.890112Z","shell.execute_reply":"2023-07-10T04:36:11.310481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.utils.plot_model(bert_model)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T04:36:54.738070Z","iopub.execute_input":"2023-07-10T04:36:54.738639Z","iopub.status.idle":"2023-07-10T04:36:56.340286Z","shell.execute_reply.started":"2023-07-10T04:36:54.738593Z","shell.execute_reply":"2023-07-10T04:36:56.338505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset():\n    def __init__(self, data, targets, tokenizer):\n        self.data = data\n        self.targets = targets\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        \n        text = self.data[idx]\n        target = self.targets[idx]\n        bert_input = self.tokenizer(text, padding='max_length', max_length = 40,\n                      truncation = True, return_tensors = 'pt')\n        \n        return bert_input, torch.tensor(target, dtype=torch.int64)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.776023Z","iopub.execute_input":"2023-06-27T09:02:18.776761Z","iopub.status.idle":"2023-06-27T09:02:18.786359Z","shell.execute_reply.started":"2023-06-27T09:02:18.776712Z","shell.execute_reply":"2023-06-27T09:02:18.785218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.20, stratify=df['target'].values)\n\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = TextDataset(train_df['text'], train_df['target'], tokenizer)\nval_dataset = TextDataset(val_df['text'], val_df['target'], tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.788072Z","iopub.execute_input":"2023-06-27T09:02:18.788492Z","iopub.status.idle":"2023-06-27T09:02:18.896328Z","shell.execute_reply.started":"2023-06-27T09:02:18.788456Z","shell.execute_reply":"2023-06-27T09:02:18.895252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size)\nval_loader = DataLoader(val_dataset, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.897766Z","iopub.execute_input":"2023-06-27T09:02:18.898531Z","iopub.status.idle":"2023-06-27T09:02:18.905297Z","shell.execute_reply.started":"2023-06-27T09:02:18.898492Z","shell.execute_reply":"2023-06-27T09:02:18.904265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.906862Z","iopub.execute_input":"2023-06-27T09:02:18.908017Z","iopub.status.idle":"2023-06-27T09:02:18.921126Z","shell.execute_reply.started":"2023-06-27T09:02:18.907980Z","shell.execute_reply":"2023-06-27T09:02:18.919729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data, labels in train_loader:\n    print(data)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.922875Z","iopub.execute_input":"2023-06-27T09:02:18.923335Z","iopub.status.idle":"2023-06-27T09:02:18.976787Z","shell.execute_reply.started":"2023-06-27T09:02:18.923300Z","shell.execute_reply":"2023-06-27T09:02:18.975729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, dropout):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(self.bert.config.hidden_size,1)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, input_id, mask):\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_out = self.dropout(pooled_output)\n        linear_out = self.linear(dropout_out)\n        final_layer = self.sigmoid(linear_out)\n        \n        return final_layer","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.979255Z","iopub.execute_input":"2023-06-27T09:02:18.979978Z","iopub.status.idle":"2023-06-27T09:02:18.987763Z","shell.execute_reply.started":"2023-06-27T09:02:18.979939Z","shell.execute_reply":"2023-06-27T09:02:18.986627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nmodel = BertClassifier(dropout=0.3)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:18.989867Z","iopub.execute_input":"2023-06-27T09:02:18.990645Z","iopub.status.idle":"2023-06-27T09:02:26.107303Z","shell.execute_reply.started":"2023-06-27T09:02:18.990606Z","shell.execute_reply":"2023-06-27T09:02:26.106164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device):\n    total_train_epoch_loss = 0\n    total_train_epoch_acc = 0\n    total_train_samples = 0\n    model.train()\n    for data, labels in dataloader:\n        input_ids = data['input_ids'].squeeze(1).to(device)\n        attention_mask = data['attention_mask'].to(device)\n        labels = labels.to(device)\n\n        output = model(input_ids, attention_mask)\n        batch_loss = criterion(output.squeeze(1), labels.float()) # removes 1 from a tensor of size(4,1), resulting in tensor size 4\n        total_train_epoch_loss +=batch_loss\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n\n        optimizer.step()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        predicted_labels = torch.round(output).squeeze(1)  # Get the predicted labels\n        batch_correct = (predicted_labels == labels).sum().item()  # Count the correct predictions\n        total_train_epoch_acc += batch_correct\n        total_train_samples += len(labels)  # Add the batch size to the total number of samples\n\n    train_epoch_acc = total_train_epoch_acc / total_train_samples\n    train_epoch_loss = total_train_epoch_loss / total_train_samples\n    return train_epoch_acc, train_epoch_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:26.113823Z","iopub.execute_input":"2023-06-27T09:02:26.114162Z","iopub.status.idle":"2023-06-27T09:02:26.124424Z","shell.execute_reply.started":"2023-06-27T09:02:26.114133Z","shell.execute_reply":"2023-06-27T09:02:26.123245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion, optimizer, device):\n    model.eval()\n    total_val_epoch_loss = 0\n    total_val_epoch_acc = 0\n    total_val_samples = 0\n    with torch.no_grad():\n        for data, labels in dataloader:\n            input_ids = data['input_ids'].squeeze(1).to(device)\n            attention_mask = data['attention_mask'].to(device)\n            labels = labels.to(device)\n\n            output = model(input_ids, attention_mask)\n            batch_loss = criterion(output.squeeze(1), labels.float()) # removes 1 from a tensor of size(4,1), resulting in tensor size 4\n            total_val_epoch_loss +=batch_loss\n            predicted_labels = torch.round(output).squeeze(1)  # Get the predicted labels\n            batch_correct = (predicted_labels == labels).sum().item()  # Count the correct predictions\n            total_val_epoch_acc += batch_correct\n            total_val_samples += len(labels)  # Add the batch size to the total number of samples\n\n    val_epoch_acc = total_val_epoch_acc / total_val_samples\n    val_epoch_loss = total_val_epoch_loss / total_val_samples\n    return val_epoch_acc, val_epoch_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:26.126212Z","iopub.execute_input":"2023-06-27T09:02:26.126592Z","iopub.status.idle":"2023-06-27T09:02:26.141517Z","shell.execute_reply.started":"2023-06-27T09:02:26.126558Z","shell.execute_reply":"2023-06-27T09:02:26.140436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 2e-5)\ncriterion.to(device)\nnum_epochs =10\nhistory = defaultdict(list)\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    train_epoch_acc, train_epoch_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n    \n    print(f'Train Accuracy:{train_epoch_acc} | Train Loss:{train_epoch_loss}')\n    \n    val_epoch_acc, val_epoch_loss = eval_model(model, val_loader, criterion, optimizer, device)\n    \n    print(f'Val Accuracy:{val_epoch_acc} | Val Loss:{val_epoch_loss}')\n    print()\n    \n    history['train_epoch_acc'].append(train_epoch_acc)\n    history['train_epoch_loss'].append(train_epoch_loss)\n    history['val_epoch_acc'].append(val_epoch_acc)\n    history['val_epoch_loss'].append(val_epoch_loss)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:02:26.142769Z","iopub.execute_input":"2023-06-27T09:02:26.144417Z","iopub.status.idle":"2023-06-27T09:08:10.750065Z","shell.execute_reply.started":"2023-06-27T09:02:26.144377Z","shell.execute_reply":"2023-06-27T09:08:10.748929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history['train_epoch_acc'], label='train accuracy')\nplt.plot(history['val_epoch_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:10.751844Z","iopub.execute_input":"2023-06-27T09:08:10.752290Z","iopub.status.idle":"2023-06-27T09:08:11.114686Z","shell.execute_reply.started":"2023-06-27T09:08:10.752252Z","shell.execute_reply":"2023-06-27T09:08:11.113753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def predictions_on_val(model, dataloader, device):\n    model.eval()\n    true_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data, labels in dataloader:\n            input_ids = data['input_ids'].squeeze(1).to(device)\n            attention_mask = data['attention_mask'].to(device)\n            labels = labels.to(device)\n            output = model(input_ids, attention_mask)\n            predicted_labels = torch.round(output).squeeze(1)  # Get the predicted labels\n            pred_labels.append(predicted_labels.cpu().numpy())\n            true_labels.append(labels.squeeze().cpu().numpy())\n            \n    true_labels = np.concatenate(true_labels)\n    pred_labels = np.concatenate(pred_labels)\n    return true_labels, pred_labels","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:11.116124Z","iopub.execute_input":"2023-06-27T09:08:11.116549Z","iopub.status.idle":"2023-06-27T09:08:11.128031Z","shell.execute_reply.started":"2023-06-27T09:08:11.116509Z","shell.execute_reply":"2023-06-27T09:08:11.126954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels, pred_labels = predictions_on_val(model, val_loader, device)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:11.129392Z","iopub.execute_input":"2023-06-27T09:08:11.130362Z","iopub.status.idle":"2023-06-27T09:08:14.376971Z","shell.execute_reply.started":"2023-06-27T09:08:11.130325Z","shell.execute_reply":"2023-06-27T09:08:14.375566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(true_labels, pred_labels, target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:14.378425Z","iopub.execute_input":"2023-06-27T09:08:14.378843Z","iopub.status.idle":"2023-06-27T09:08:14.403756Z","shell.execute_reply.started":"2023-06-27T09:08:14.378803Z","shell.execute_reply":"2023-06-27T09:08:14.402588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0)\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=0)\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');\ncm = confusion_matrix(true_labels, pred_labels)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:14.405226Z","iopub.execute_input":"2023-06-27T09:08:14.406352Z","iopub.status.idle":"2023-06-27T09:08:14.743151Z","shell.execute_reply.started":"2023-06-27T09:08:14.406306Z","shell.execute_reply":"2023-06-27T09:08:14.742200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tweet = test_df['text'][random.randint(0, len(test_df))]\nbert_input = tokenizer(test_tweet, padding='max_length', max_length = 40,\n                      truncation = True, return_tensors = 'pt')","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:14.744711Z","iopub.execute_input":"2023-06-27T09:08:14.745381Z","iopub.status.idle":"2023-06-27T09:08:14.752956Z","shell.execute_reply.started":"2023-06-27T09:08:14.745342Z","shell.execute_reply":"2023-06-27T09:08:14.751939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = bert_input['input_ids'].squeeze(1).to(device)\nattention_mask = bert_input['attention_mask'].to(device)\nlabels = labels.to(device)\noutput = model(input_ids, attention_mask)\npredicted_labels = torch.round(output).squeeze(1).item()\nprint('Tweet:', test_tweet)\nprint()\nsentiment = 'Not a disaster' if predicted_labels==0 else 'Disaster'\nprint('Predicted Sentiment:', sentiment)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:14.754503Z","iopub.execute_input":"2023-06-27T09:08:14.754842Z","iopub.status.idle":"2023-06-27T09:08:14.787273Z","shell.execute_reply.started":"2023-06-27T09:08:14.754809Z","shell.execute_reply":"2023-06-27T09:08:14.786262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"targets = []\nfor text in test_df['text']:\n    bert_input = tokenizer(text, padding='max_length', max_length = 40,\n                      truncation = True, return_tensors = 'pt')\n    input_ids = bert_input['input_ids'].squeeze(1).to(device)\n    attention_mask = bert_input['attention_mask'].to(device)\n    labels = labels.to(device)\n    output = model(input_ids, attention_mask)\n    predicted_labels = torch.round(output).squeeze(1).item()\n    targets.append(predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:08:14.789014Z","iopub.execute_input":"2023-06-27T09:08:14.789423Z","iopub.status.idle":"2023-06-27T09:09:02.098341Z","shell.execute_reply.started":"2023-06-27T09:08:14.789387Z","shell.execute_reply":"2023-06-27T09:09:02.097264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(columns = ['id', 'target'])\nsub['id'] = test_df.id\nsub['target'] = targets","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:09:02.099845Z","iopub.execute_input":"2023-06-27T09:09:02.100363Z","iopub.status.idle":"2023-06-27T09:09:02.114258Z","shell.execute_reply.started":"2023-06-27T09:09:02.100322Z","shell.execute_reply":"2023-06-27T09:09:02.113248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T09:09:02.115582Z","iopub.execute_input":"2023-06-27T09:09:02.116553Z","iopub.status.idle":"2023-06-27T09:09:02.142486Z","shell.execute_reply.started":"2023-06-27T09:09:02.116516Z","shell.execute_reply":"2023-06-27T09:09:02.141411Z"},"trusted":true},"execution_count":null,"outputs":[]}]}